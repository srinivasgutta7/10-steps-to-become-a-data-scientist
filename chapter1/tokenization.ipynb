{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinivasgutta7/10-steps-to-become-a-data-scientist/blob/master/chapter1/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f137fc7-93e8-4600-973b-f01ab01b2806",
      "metadata": {
        "id": "8f137fc7-93e8-4600-973b-f01ab01b2806",
        "outputId": "5c4a5d4b-0caf-40fb-bebd-8a156296d1e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] the quick brown fox jumps over the lazy dog! [SEP]\n",
            "[101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 999, 102]\n",
            "[CLS], the, quick, brown, fox, jumps, over, the, lazy, dog, !, [SEP]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"  # deepseek-ai/DeepSeek-V2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize a text\n",
        "text = \"The quick brown fox jumps over the lazy dog!\"\n",
        "\n",
        "encoded_text = tokenizer(text)\n",
        "\n",
        "# Access the tokenized IDs\n",
        "print(tokenizer.decode(encoded_text[\"input_ids\"]))\n",
        "\n",
        "print(encoded_text[\"input_ids\"])\n",
        "print(\", \".join([tokenizer.decode(t) for t in encoded_text[\"input_ids\"]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb08b88-680e-45f1-b75a-c0f3d757d357",
      "metadata": {
        "id": "2bb08b88-680e-45f1-b75a-c0f3d757d357"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " # Initialize the LLM with OpenAI's model\n",
        " llm = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"gpt-4\",\n",
        "temperature=0.5)\n",
        " template = \"\"\"\n",
        " As a futuristic poet, I want to write a poem that captures the essence of\n",
        "{emotion}.\n",
        " Can you suggest a title for a poem about {emotion} set in the year {year}?\n",
        " \"\"\"\n",
        " prompt = PromptTemplate(\n",
        " input_variables=[\"emotion\", \"year\"],\n",
        " template=template,\n",
        " )\n",
        " # Input data for the prompt\n",
        " input_data = {\"emotion\": \"solitude\", \"year\": \"2500\"}\n",
        " chain = prompt | llm\n",
        " response = chain.invoke(input_data)\n",
        "\n",
        " print(\"Emotion: solitude\")\n",
        " print(\"Year: 2500\")\n",
        " print(\"AI-generated poem title:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        " from langchain_core.prompts.prompt import PromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " # Initialize the language model with specific settings\n",
        " language_model = ChatOpenAI(\n",
        " api_key=\"sk-proj- 056py5goMfqp8_g2gOgfhefr1HLriyWyP6erQJ4dQyi3D2HWBxJgCW\n",
        "rjWMbvMTJdvxHlzaWm11T3BlbkFJss1mhhNZJ7YREWFugP2wKQoMHIR3FMCDZxiOA_rPSrC\n",
        "fXZK6ZJbcGJ85dpMGV4adCt7R_zrUkA\",\n",
        " model_name=\"gpt-4o-mini\",\n",
        " temperature=0\n",
        " )\n",
        "\n",
        " # Sample color-to-emotion associations\n",
        " color_emotion_pairs = [\n",
        " {\"color\": \"red\", \"emotion\": \"energy\"},\n",
        " {\"color\": \"blue\", \"emotion\": \"peace\"},\n",
        " {\"color\": \"green\", \"emotion\": \"growth\"},\n",
        " ]\n",
        " # Template for formatting examples in a structured way\n",
        " example_structure = \"\"\"\n",
        " Color: {color}\n",
        " Associated Emotion: {emotion}\\n\n",
        " \"\"\"\n",
        " # Create the example prompt template\n",
        " color_prompt_template = PromptTemplate(\n",
        " input_variables=[\"color\", \"emotion\"],\n",
        " template=example_structure,\n",
        " )\n",
        " # Construct a few-shot prompt template using the color-emotion pairs\n",
        " few_shot_color_prompt = FewShotPromptTemplate(examples=color_emotion_pairs,\n",
        " example_prompt=color_prompt_template,\n",
        " prefix=\"Here are a few examples demonstrating the emotions linked with colors:\\n\\n\",\n",
        " suffix=\"\\n\\nNow, considering the new color, predict the associated emotion:\\n\\nColor: {input}\\nEmotion:\",input_variables=[\"input\"],example_separator=\"\\n\",\n",
        " )\n",
        " # Generate the final prompt for a new color input\n",
        " final_prompt_text = few_shot_color_prompt.format(input=\"purple\")\n",
        " # Use the generated prompt and run it through the language model\n",
        " final_prompt = PromptTemplate(template=final_prompt_text, input_\n",
        " variables=[])\n",
        " prompt_chain = final_prompt | language_model\n",
        "\n",
        " # Get the AI-generated response for the input color\n",
        " model_output = prompt_chain.invoke({})\n",
        " # Print the input color and its corresponding predicted emotion\n",
        " print(\"Color: purple\")\n",
        " print(\"Predicted Emotion:\", model_output.content)"
      ],
      "metadata": {
        "id": "TSf-1bO65H6S"
      },
      "id": "TSf-1bO65H6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " # Initialize the language model\n",
        " llm = ChatOpenAI(api_key=\"sk-proj- 056py5goMfqp8_g2gOgfhefr1HLriyWyP6erQJ4dQ\n",
        "yi3D2HWBxJgCWrjWMbvMTJdvxHlzaWm11T3BlbkFJss1mhhNZJ7YREWFugP2wKQoMHIR3FMCDZx\n",
        "iOA_rPSrCfXZK6ZJbcGJ85dpMGV4adCt7R_zrUkA\",\n",
        " model_name=\"gpt-4o-mini\",\n",
        " temperature=0)\n",
        " # Prompt 1: Ask for the scientist who developed the theory of general relativity\n",
        " question_template = \"\"\"Who is the scientist that formulated the theory of\n",
        "general relativity?\n",
        " Answer: \"\"\"\n",
        " prompt_for_scientist = PromptTemplate(template=question_template, input_variables=[])\n",
        "\n",
        " # Prompt 2: Ask for a brief explanation of the scientist's theory of\n",
        "general relativity\n",
        " fact_template = \"\"\"Give a brief explanation of {scientist}'s theory of\n",
        "general relativity.\n",
        " Answer: \"\"\"\n",
        " prompt_for_fact = PromptTemplate(input_variables=[\"scientist\"], template=fact_template)\n",
        " # Create a runnable chain for the first prompt to retrieve the scientist's name\n",
        " chain_for_question = prompt_for_scientist | llm\n",
        " # Get the response for the first question\n",
        " response_to_question = chain_for_question.invoke({})\n",
        " # Extract the scientist's name from the response\n",
        " scientist_name = response_to_question.content.strip()\n",
        " # Create a runnable chain for the second prompt using the extracted scientist's name\n",
        " chain_for_fact = prompt_for_fact | llm\n",
        " # Input data for the second prompt\n",
        " fact_input = {\"scientist\": scientist_name}\n",
        " # Get the response for the second question about the theory\n",
        " response_to_fact = chain_for_fact.invoke(fact_input)\n",
        " # Output the scientist's name and the explanation of their theory\n",
        " print(\"Scientist:\", scientist_name)\n",
        " print(\"Theory Description:\", response_to_fact)"
      ],
      "metadata": {
        "id": "DzmO4s-37CeD"
      },
      "id": "DzmO4s-37CeD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install langchain_community\n",
        " from\tlangchain_community.llms\timport\tFakeListLLM"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ciAzSzToBFfZ",
        "outputId": "1c45ef55-2238-4095-8694-9b4f32bcf0f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ciAzSzToBFfZ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.26 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        " from langchain.prompts import PromptTemplate\n",
        " from langchain_openai import OpenAI\n",
        " # Step 1: Define the language model (in this case, OpenAI's GPT)\n",
        " llm = OpenAI(api_key=\"sk-proj- 056py5goMfqp8_g2gOgfhefr1HLriyWyP6erQJ4dQyi3D\n",
        "2HWBxJgCWrjWMbvMTJdvxHlzaWm11T3BlbkFJss1mhhNZJ7YREWFugP2wKQoMHIR3FMCDZxiOA_\n",
        "rPSrCfXZK6ZJbcGJ85dpMGV4adCt7R_zrUkA\",\n",
        " temperature=0.7)  # Set the desired temperature for response variability\n",
        " # Step 2: Define the prompt template\n",
        " prompt_template = \"\"\"\n",
        " Summarize the following question briefly:\n",
        " {user_question}\n",
        " \"\"\"\n",
        " # Step 3: Create the PromptTemplate object\n",
        " prompt = PromptTemplate(\n",
        " input_variables=[\"user_question\"],\n",
        " template=prompt_template,\n",
        " )\n",
        " # Step 4: Create the LLMChain using the language model and prompt template\n",
        " chain = prompt | llm\n",
        " # Step 5: Input the user's question and run the chain\n",
        " user_question = \"Can you explain how photosynthesis works in simple terms?\"\n",
        " output = chain.invoke(user_question)\n",
        " # Print the summarized question\n",
        " print(\"Summarized Question:\", output)"
      ],
      "metadata": {
        "id": "1bnAQKtWPrzL"
      },
      "id": "1bnAQKtWPrzL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from langchain.chains import LLMChain, SimpleSequentialChain\n",
        " from langchain.prompts import PromptTemplate\n",
        " from langchain_openai import OpenAI\n",
        " # Step 1: Define the language model\n",
        " llm = OpenAI(api_key=\"sk-proj- 056py5goMfqp8_g2gOgfhefr1HLriyWyP6erQJ4dQyi3D\n",
        "2HWBxJgCWrjWMbvMTJdvxHlzaWm11T3BlbkFJss1mhhNZJ7YREWFugP2wKQoMHIR3FMCDZxiOA_\n",
        "rPSrCfXZK6ZJbcGJ85dpMGV4adCt7R_zrUkA\",\n",
        " temperature=0.7)\n",
        " # Step 2: Create the first prompt template to summarize the question\n",
        " summary_prompt_template = \"\"\"\n",
        " Summarize the following question briefly:\n",
        " {user_question}\n",
        " \"\"\"\n",
        " # Step 3: Create the second prompt template to generate a short answer\n",
        " answer_prompt_template = \"\"\"\n",
        " Provide a brief answer to the following question:\n",
        " {summarized_question}\n",
        " \"\"\"\n",
        " # Step 4: Create PromptTemplate objects for both prompts\n",
        " summary_prompt = PromptTemplate(input_variables=[\"user_question\"],template=summary_prompt_template,)\n",
        " answer_prompt = PromptTemplate(\n",
        " input_variables=[\"summarized_question\"],\n",
        " template=answer_prompt_template,\n",
        " )\n",
        " # Step 5: Create LLMChain objects for both steps\n",
        " summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
        " answer_chain = LLMChain(llm=llm, prompt=answer_prompt)\n",
        " # Step 6: Create a SimpleSequentialChain that links both chains together\n",
        " sequential_chain = SimpleSequentialChain(\n",
        " chains=[summary_chain, answer_chain]\n",
        " )\n",
        " # Step 7: Input the user's question and run the sequential chain\n",
        " user_question = \"Can you explain how photosynthesis works in simple terms?\"\n",
        " output = sequential_chain.run(user_question)\n",
        " # Print the output of the sequential chain\n",
        " print(\"Final Output:\", output)"
      ],
      "metadata": {
        "id": "mnAGDSkQP4TM"
      },
      "id": "mnAGDSkQP4TM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        " # Set your OpenAI API key\n",
        " os.environ[\"OPENAI_API_KEY\"] = \"sk-proj- 056py5goMfqp8_g2gOgfhefr1HLriyWyP6\n",
        "erQJ4dQyi3D2HWBxJgCWrjWMbvMTJdvxHlzaWm11T3BlbkFJss1mhhNZJ7YREWFugP2wKQoMHI\n",
        "R3FMCDZxiOA_rPSrCfXZK6ZJbcGJ85dpMGV4adCt7R_zrUkA\"\n",
        " from langchain_core.output_parsers import StrOutputParser\n",
        " from langchain_core.prompts import ChatPromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " from langchain.memory import ConversationBufferMemory\n",
        " from langchain.chains import LLMChain\n",
        " # Step 1: Define a prompt template for conversation, using a variable for user input\n",
        " prompt = ChatPromptTemplate.from_messages([(\"user\", \"{user_input}\")])\n",
        " # Step 2: Set up the ChatOpenAI model (gpt-3.5-turbo in this case) with temperature control\n",
        " llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        " # Step 3: Create memory to store conversation history\n",
        " memory = ConversationBufferMemory()\n",
        " # Step 4: Create the chain combining prompt, model, and output parser\n",
        " chain = LLMChain(prompt=prompt, llm=llm, memory=memory, output_parser=StrOutputParser())\n",
        " # Simulate a conversation by invoking the chain with memory\n",
        " # First user input\n",
        " response_1 = chain.invoke({\"user_input\": \"Can you explain what photosynthesis is?\"})\n",
        " print(\"AI Response 1:\", response_1)\n",
        " # Second user input\n",
        " response_2 = chain.invoke({\"user_input\": \"What happens during the light\n",
        "dependent reactions?\"})\n",
        " print(\"AI Response 2:\", response_2)\n",
        " # Third user input\n",
        " response_3 = chain.invoke({\"user_input\": \"Can you summarize both for me?\"})\n",
        " print(\"AI Response 3:\", response_3)\n",
        " print(memory)"
      ],
      "metadata": {
        "id": "KDCdK1B4RsIs"
      },
      "id": "KDCdK1B4RsIs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from langchain_core.prompts import ChatPromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " from langchain_core.output_parsers import StrOutputParser\n",
        " from langchain.chains import SimpleSequentialChain\n",
        " # Step 1: Define the first prompt to accept a question and context\n",
        " question_prompt = ChatPromptTemplate.from_messages(\n",
        " [(\"user\", \"Given the context: '{context}', answer the question: '{question}'\")])\n",
        " # Step 2: Define the ChatOpenAI model\n",
        " llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        " # Step 3: Create the output parser\n",
        " output_parser = StrOutputParser()\n",
        " # Step 4: Combine the prompt and model into a chain\n",
        " # This is a simple chain that handles multiple inputs (question and context)\n",
        " chain = question_prompt | llm | output_parser\n",
        " # Step 5: Define the inputs for the multi-input chain\n",
        " inputs = {\n",
        " \"question\": \"How does photosynthesis work?\",\n",
        " \"context\": \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\"}\n",
        " # Step 6: Run the chain with both inputs\n",
        " response = chain.invoke(inputs)\n",
        " # Output the response\n",
        " print(\"Response:\", response)"
      ],
      "metadata": {
        "id": "qUI6ylsGTdv0"
      },
      "id": "qUI6ylsGTdv0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from langchain_core.prompts import ChatPromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " from langchain_core.output_parsers import StrOutputParser\n",
        " from langchain.chains import LLMChain\n",
        " from langchain.chains import SequentialChain\n",
        " # Step 1: Define the prompt for generating a summary\n",
        " summary_prompt = ChatPromptTemplate.from_messages(\n",
        " [(\"user\", \"Please summarize the following text: {input_text}\")]\n",
        " )\n",
        " # Step 2: Define the prompt for extracting key points\n",
        " key_points_prompt = ChatPromptTemplate.from_messages(\n",
        " [(\"user\", \"Extract the key points from the following text: {input_\n",
        " text}\")]\n",
        " )\n",
        " # Step 3: Set up the ChatOpenAI model (same model for both tasks)\n",
        " llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        " # Step 4: Create the output parser\n",
        " output_parser = StrOutputParser()\n",
        " # Step 5: Create LLMChain for summarization and key point extraction\n",
        " summary_chain = LLMChain(prompt=summary_prompt, llm=llm, output_\n",
        " key=\"summary\")  # Changed output key to \"summary\"\n",
        " key_points_chain = LLMChain(prompt=key_points_prompt, llm=llm, output_\n",
        " key=\"key_points\")  # Changed output key to \"key_points\"\n",
        " # Step 6: Create a SequentialChain that runs both chains (true\n",
        "multi-output)\n",
        " multi_output_chain = SequentialChain(\n",
        " chains=[summary_chain, key_points_chain],\n",
        " 35\n",
        "Chapter 1  LangChain and python: BasiCs\n",
        " input_variables=[\"input_text\"],  # single input passed to both chains\n",
        " output_variables=[\"summary\", \"key_points\"]  # two outputs\n",
        " )\n",
        " # Step 7: Define the input text\n",
        " input_text = \"\"\"\n",
        " Photosynthesis is a process used by plants to convert light energy into\n",
        "chemical energy. During photosynthesis,\n",
        " plants take in carbon dioxide (CO2) and water (H2O) from the air and soil.\n",
        "Within the plant cell, the water is oxidized,\n",
        " meaning it loses electrons, while the carbon dioxide is reduced, meaning it\n",
        "gains electrons. This process converts\n",
        " the water into oxygen and the carbon dioxide into glucose. The plant then\n",
        "releases the oxygen back into the air,\n",
        " and stores energy in the form of glucose molecules.\n",
        " \"\"\"\n",
        " # Step 8: Run the multi-output chain using apply() for multiple outputs\n",
        " outputs = multi_output_chain.apply([{\"input_text\": input_text}])[0]\n",
        " # Step 9: Output the responses\n",
        " print(\"Summary:\", outputs['summary'])\n",
        " print(\"Key Points:\", outputs['key_points'])"
      ],
      "metadata": {
        "id": "x-q3WMRYfwjz"
      },
      "id": "x-q3WMRYfwjz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}