{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f137fc7-93e8-4600-973b-f01ab01b2806",
      "metadata": {
        "id": "8f137fc7-93e8-4600-973b-f01ab01b2806",
        "outputId": "5c4a5d4b-0caf-40fb-bebd-8a156296d1e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] the quick brown fox jumps over the lazy dog! [SEP]\n",
            "[101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 999, 102]\n",
            "[CLS], the, quick, brown, fox, jumps, over, the, lazy, dog, !, [SEP]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B\"  # deepseek-ai/DeepSeek-V2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize a text\n",
        "text = \"The quick brown fox jumps over the lazy dog!\"\n",
        "\n",
        "encoded_text = tokenizer(text)\n",
        "\n",
        "# Access the tokenized IDs\n",
        "print(tokenizer.decode(encoded_text[\"input_ids\"]))\n",
        "\n",
        "print(encoded_text[\"input_ids\"])\n",
        "print(\", \".join([tokenizer.decode(t) for t in encoded_text[\"input_ids\"]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb08b88-680e-45f1-b75a-c0f3d757d357",
      "metadata": {
        "id": "2bb08b88-680e-45f1-b75a-c0f3d757d357"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " # Initialize the LLM with OpenAI's model\n",
        " llm = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"gpt-4\",\n",
        "temperature=0.5)\n",
        " template = \"\"\"\n",
        " As a futuristic poet, I want to write a poem that captures the essence of\n",
        "{emotion}.\n",
        " Can you suggest a title for a poem about {emotion} set in the year {year}?\n",
        " \"\"\"\n",
        " prompt = PromptTemplate(\n",
        " input_variables=[\"emotion\", \"year\"],\n",
        " template=template,\n",
        " )\n",
        " # Input data for the prompt\n",
        " input_data = {\"emotion\": \"solitude\", \"year\": \"2500\"}\n",
        " chain = prompt | llm\n",
        " response = chain.invoke(input_data)\n",
        "\n",
        " print(\"Emotion: solitude\")\n",
        " print(\"Year: 2500\")\n",
        " print(\"AI-generated poem title:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        " from langchain_core.prompts.prompt import PromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " # Initialize the language model with specific settings\n",
        " language_model = ChatOpenAI(\n",
        " api_key=\"sk-proj- 056py5goMfqp8_g2gOgfhefr1HLriyWyP6erQJ4dQyi3D2HWBxJgCW\n",
        "rjWMbvMTJdvxHlzaWm11T3BlbkFJss1mhhNZJ7YREWFugP2wKQoMHIR3FMCDZxiOA_rPSrC\n",
        "fXZK6ZJbcGJ85dpMGV4adCt7R_zrUkA\",\n",
        " model_name=\"gpt-4o-mini\",\n",
        " temperature=0\n",
        " )\n",
        "\n",
        " # Sample color-to-emotion associations\n",
        " color_emotion_pairs = [\n",
        " {\"color\": \"red\", \"emotion\": \"energy\"},\n",
        " {\"color\": \"blue\", \"emotion\": \"peace\"},\n",
        " {\"color\": \"green\", \"emotion\": \"growth\"},\n",
        " ]\n",
        " # Template for formatting examples in a structured way\n",
        " example_structure = \"\"\"\n",
        " Color: {color}\n",
        " Associated Emotion: {emotion}\\n\n",
        " \"\"\"\n",
        " # Create the example prompt template\n",
        " color_prompt_template = PromptTemplate(\n",
        " input_variables=[\"color\", \"emotion\"],\n",
        " template=example_structure,\n",
        " )\n",
        " # Construct a few-shot prompt template using the color-emotion pairs\n",
        " few_shot_color_prompt = FewShotPromptTemplate(examples=color_emotion_pairs,\n",
        " example_prompt=color_prompt_template,\n",
        " prefix=\"Here are a few examples demonstrating the emotions linked with colors:\\n\\n\",\n",
        " suffix=\"\\n\\nNow, considering the new color, predict the associated emotion:\\n\\nColor: {input}\\nEmotion:\",input_variables=[\"input\"],example_separator=\"\\n\",\n",
        " )\n",
        " # Generate the final prompt for a new color input\n",
        " final_prompt_text = few_shot_color_prompt.format(input=\"purple\")\n",
        " # Use the generated prompt and run it through the language model\n",
        " final_prompt = PromptTemplate(template=final_prompt_text, input_\n",
        " variables=[])\n",
        " prompt_chain = final_prompt | language_model\n",
        "\n",
        " # Get the AI-generated response for the input color\n",
        " model_output = prompt_chain.invoke({})\n",
        " # Print the input color and its corresponding predicted emotion\n",
        " print(\"Color: purple\")\n",
        " print(\"Predicted Emotion:\", model_output.content)"
      ],
      "metadata": {
        "id": "TSf-1bO65H6S"
      },
      "id": "TSf-1bO65H6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        " from langchain_openai import ChatOpenAI\n",
        " # Initialize the language model\n",
        " llm = ChatOpenAI(api_key=\"sk-proj- 056py5goMfqp8_g2gOgfhefr1HLriyWyP6erQJ4dQ\n",
        "yi3D2HWBxJgCWrjWMbvMTJdvxHlzaWm11T3BlbkFJss1mhhNZJ7YREWFugP2wKQoMHIR3FMCDZx\n",
        "iOA_rPSrCfXZK6ZJbcGJ85dpMGV4adCt7R_zrUkA\",\n",
        " model_name=\"gpt-4o-mini\",\n",
        " temperature=0)\n",
        " # Prompt 1: Ask for the scientist who developed the theory of general relativity\n",
        " question_template = \"\"\"Who is the scientist that formulated the theory of\n",
        "general relativity?\n",
        " Answer: \"\"\"\n",
        " prompt_for_scientist = PromptTemplate(template=question_template, input_variables=[])\n",
        "\n",
        " # Prompt 2: Ask for a brief explanation of the scientist's theory of\n",
        "general relativity\n",
        " fact_template = \"\"\"Give a brief explanation of {scientist}'s theory of\n",
        "general relativity.\n",
        " Answer: \"\"\"\n",
        " prompt_for_fact = PromptTemplate(input_variables=[\"scientist\"], template=fact_template)\n",
        " # Create a runnable chain for the first prompt to retrieve the scientist's name\n",
        " chain_for_question = prompt_for_scientist | llm\n",
        " # Get the response for the first question\n",
        " response_to_question = chain_for_question.invoke({})\n",
        " # Extract the scientist's name from the response\n",
        " scientist_name = response_to_question.content.strip()\n",
        " # Create a runnable chain for the second prompt using the extracted scientist's name\n",
        " chain_for_fact = prompt_for_fact | llm\n",
        " # Input data for the second prompt\n",
        " fact_input = {\"scientist\": scientist_name}\n",
        " # Get the response for the second question about the theory\n",
        " response_to_fact = chain_for_fact.invoke(fact_input)\n",
        " # Output the scientist's name and the explanation of their theory\n",
        " print(\"Scientist:\", scientist_name)\n",
        " print(\"Theory Description:\", response_to_fact)"
      ],
      "metadata": {
        "id": "DzmO4s-37CeD"
      },
      "id": "DzmO4s-37CeD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}